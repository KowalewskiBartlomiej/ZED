{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f0811dab-f855-475a-a7ab-2dfce0f5a2be",
   "metadata": {},
   "source": [
    "# Retrieval Augmented Generation\n",
    "\n",
    "Ten notatnik pomoże Ci zapoznać się z podstawowym podejściem do Retrieval Augmented Generation (RAG). W trakcie ćwiczenia będziemy korzystać głównie z bibliotek [openai](https://github.com/openai/openai-python), [langchain](https://python.langchain.com/) i [trulens](https://www.trulens.org/). Po uzupełnieniu tego notatnika powinieneś wiedzieć:\n",
    "- czym jest RAG,\n",
    "- jak z poziomu kodu komunikować się z LLMem,\n",
    "- jak pobierać i dzielić dokumenty tekstowe na potrzeby RAG,\n",
    "- jak policzyć zanurzenia i przechowywać zanurzenia dla swoich dokumentów,\n",
    "- jak wykonać sematyczne wyszukiwanie wśród dokumentów,\n",
    "- jak wstrzykiwać kontekst do zapytań do LLMów,\n",
    "- jak prowadzić dłuższe konwersacje z LLMem,\n",
    "- jak mierzyć i monitorować jakość odpowiedzi LLMów.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6735ec64-5f8e-42de-903c-2584069a847c",
   "metadata": {},
   "source": [
    "## Przygotowanie\n",
    "\n",
    "Do pracy z LLMami potrzebny będzie klucz API do dostawcy takowych. Na tych zajęciach będziemy łączyć się z modelami firmy OpenAI. Jeśli jeszcze nie masz klucza API, skorzystaj z instrukcji na stronie: https://platform.openai.com/docs/quickstart?context=python.\n",
    "\n",
    "Oprócz tego będziemy potrzebować kilku bibliotek:\n",
    "\n",
    "```{command}\n",
    "pip install openai langchain langchain_openai pypdf youtube-transcript-api chromadb trulens_eval\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3da20b87-4213-4184-9ed8-a672b5559454",
   "metadata": {},
   "source": [
    "Jeśli masz potrzebne biblioteki, możesz sprawdzić czy jesteś w stanie komunikować się z LLMem. Wykonaj poniższy kod, żeby sprawdzić czy wszystko działa. Jeśli nie masz klucza API w zmiennej systemowej, po prostu wklej go do kodu poniżej zamiast odwołania do zmiennej systemowej. Jeśli chcesz pracować ze zmienną systemową, prawdopodobnie będziesz musiał po dodaniu zmiennej systemowej uruchomić nowy terminal, w którym nowa zmienna będzie widoczna, ponownie odpalić serwer jupyter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a33419a0-a224-4a49-b725-fe8c7940f14a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-08T07:40:04.119346700Z",
     "start_time": "2024-01-08T07:40:02.053850100Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "import numpy as np\n",
    "openai.api_key = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e97f30f0-2627-46ea-a00e-b7ecd923cbb1",
   "metadata": {},
   "source": [
    "**Zad. 1: Uruchom poniższy kod. Możesz zmodyfikować opis systemu i zapytanie.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8986db69-97a4-4db4-b1b0-d6ac8dbae779",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = openai.OpenAI()\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "  model=\"gpt-3.5-turbo\",\n",
    "  messages=[\n",
    "    {\"role\": \"system\", \"content\": \"You are a poetic assistant, skilled in songwriting.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Write lyrics for a country song about Polish winters.\"}\n",
    "  ]\n",
    ")\n",
    "\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "703e1c1e-1827-4a69-8b28-1f439c01b1d9",
   "metadata": {},
   "source": [
    "_Jeśli masz problemy z uzyskaniem odpowiedzi, sprawdź czy masz źródła na koncie powiązanym z kluczem API: https://platform.openai.com/account/billing/overview. Na tej samej stronie możesz zobaczyć ile zostało Ci funduszy. Podczas tych zajęć nie powinniśmy wydać więcej niż kilkanaście centów._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "605b30d7-34ba-46de-adca-f2283f69e9cb",
   "metadata": {},
   "source": [
    "Oprócz informacji zwrotnej od LLMa mamy również szereg metadanych, w tym ile tokenów wysłaliśmy a ile odebraliśmy.\n",
    "\n",
    "**Zad. 2: Wiedząc, że dla GPT-3.5-turbo koszt wysyłanych danych to 0.001 USD/1K tokenów, a koszt odpowiedzi to 0.002 USD/1K tokenów, policz ile kosztowało Cię powyższe zapytanie korzystając z pola `completion.usage`. _Jeśli ktoś chce korzystać z GPT-4 Turbo (gpt-4-1106-preview) to koszty wysłanych i odebranych tokenów to kolejno 0.01 i 0.03 USD/1K tokenów._**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8da42b1-5b43-49dc-b15b-8f3e6aa26f04",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5a6b6273-459d-49aa-9faa-53016a9f3b20",
   "metadata": {},
   "source": [
    "Na koniec zapiszemy sobie treść wyniku, może się jeszcze przyda."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a9b290a-5cd3-48f7-9479-046bd6cc06ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "song = completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16a5fa74-31cb-4552-aa04-94eda6640b30",
   "metadata": {},
   "source": [
    "## Wgrywanie danych\n",
    "\n",
    "Langchain (jak i inne narzędzia, np. LlamaIndex) posiadają szereg funkcji pomocniczych do zdobywania tekstu z filmów, podkastów, baz danych, stron internetowych i innych źródeł. Na potrzeby tych zajęć, jako kontekst wykorzystamy dwie krótkie książki Andrew Ng w formacie PDF i napisy z dwóch filmów na Youtube."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3b991b89-b0e5-4afb-8524-4d1537e97b35",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-08T07:44:28.160405800Z",
     "start_time": "2024-01-08T07:44:19.570504400Z"
    }
   },
   "outputs": [],
   "source": [
    "# Wczytujemy książki\n",
    "from langchain.document_loaders import PyPDFDirectoryLoader\n",
    "loader = PyPDFDirectoryLoader(\"./books\")\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2f761f8-5f35-4ae5-8f29-a0f911fab497",
   "metadata": {},
   "source": [
    "PDFy są domyślnie dzielone na strony, zatem zmienna `docs` to lista stron. Zobaczmy ile stron mają łącznie obie książki i co jest na stronie 23."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bbb08129-fead-4a79-a807-65de986b90f0",
   "metadata": {
    "scrolled": true,
    "ExecuteTime": {
     "end_time": "2024-01-08T07:44:28.177407200Z",
     "start_time": "2024-01-08T07:44:28.168426700Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Łacznie w książka jest 159 stron.\n",
      "\n",
      "Zawartość 23. strony w kolekcji to:\n",
      "PAGE 23Each project is only one step on a longer journey, hopefully one that has a positive impact. In addition:\n",
      "Don’t worry about starting too small. One of my first machine learning research projects involved \n",
      "training a neural network to see how well it could mimic the sin(x) function. It wasn’t very useful, but \n",
      "was a great learning experience that enabled me to move on to bigger projects.\n",
      "Building a portfolio of projects, especially one \n",
      "that shows progress over time from simple to \n",
      "complex undertakings, will be a big help when \n",
      "it comes to looking for a job.Communication is key.  You need to be able to explain your thinking if you want others to see \n",
      "the value in your work and trust you with resources that you can invest in larger projects. To get \n",
      "a project started, communicating the value of what you hope to build will help bring colleagues, \n",
      "mentors, and managers onboard — and help them point out flaws in your reasoning. After you’ve \n",
      "finished, the ability to explain clearly what you accomplished will help convince others to open the \n",
      "door to larger projects.\n",
      "Leadership isn’t just for managers.  When you reach the point of working on larger AI projects that \n",
      "require teamwork, your ability to lead projects will become more important, whether or not you are \n",
      "in a formal position of leadership. Many of my friends have successfully pursued a technical rather \n",
      "than managerial career, and their ability to help steer a project by applying deep technical insights \n",
      "— for example, when to invest in a new technical architecture or collect more data of a certain type \n",
      "— allowed them to grow as leaders and also helped significantly improve the project.Building a Portfolio of Projects That Shows Skill Progression CHAPTER 6\n"
     ]
    }
   ],
   "source": [
    "print(f\"Łacznie w książka jest {len(docs)} stron.\")\n",
    "print()\n",
    "print(f\"Zawartość 23. strony w kolekcji to:\\n{docs[22].page_content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff312339-0bfd-47f7-8b79-f13ce7c32e5e",
   "metadata": {},
   "source": [
    "Teraz pobierzemy napisy z dwóch filmików gdzie przemawia Andrew Ng. Zwróć uwagę na parametr language - pozwala nam on priorytetyzować ręcznie sporządzone napisy dostarczone przez twórcę filmu (automatycznie wygenerowane napisy w przypadku pierwszego filmu nie są idealne...)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9f520c10-16f0-4a8d-9772-6677713d2187",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-08T07:47:43.626961300Z",
     "start_time": "2024-01-08T07:47:39.018360400Z"
    }
   },
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import YoutubeLoader\n",
    "clips = []\n",
    "\n",
    "for link in [\"https://www.youtube.com/watch?v=5p248yoa3oE\",\n",
    "             \"https://www.youtube.com/watch?v=0jspaMLxBig\"]:\n",
    "    loader = YoutubeLoader.from_youtube_url(link, add_video_info=False, language=[\"en-US\", \"en-GB\", \"en\"])\n",
    "    clips.extend(loader.load())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d35e3d47-fc72-4fd7-bcf8-0f25a701a9c0",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "**Zad. 3: Zobacz ile fragmentów (obiektów typu Document) mają łącznie teksty obu filmików i co jest w transkrypcie o indeksie 0.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0b4e826d-68e3-41cf-9f03-ac192f3790e3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-08T07:47:43.656907300Z",
     "start_time": "2024-01-08T07:47:43.627963800Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Łacznie w książka jest 2 transkryptów stron.\n",
      "\n",
      "Pierwszy transkrypt w kolekcji to:\n",
      "[MUSIC PLAYING] It is my pleasure to welcome\n",
      "Dr. Andrew Ng, tonight. Andrew is the managing\n",
      "general partner of AI Fund, founder of DeepLearning.AI\n",
      "and Landing AI, chairman and\n",
      "co-founder of Coursera, and an adjunct professor\n",
      "of Computer Science, here at Stanford. Previously, he had started\n",
      "and led the Google Brain team, which had helped\n",
      "Google adopt modern AI. And he was also director\n",
      "of the Stanford AI lab. About eight million people, 1\n",
      "in 1,000 persons on the planet, have taken an AI class from him. And through, both, his\n",
      "education and his AI work, he has changed numerous lives. Please welcome Dr. Andrew Ng. [APPLAUSE] Thank you, Lisa. It's good to see everyone. So, what I want to do\n",
      "today is chat to you about some opportunities in AI. So I've been saying AI\n",
      "is a new electricity. One of the difficult things\n",
      "to understand about AI is that it is a general\n",
      "purpose technology, meaning that it's not\n",
      "useful only for one thing but it's useful for lots\n",
      "of different applications, kind of like electricity. If I were to ask you, what\n",
      "is electricity good for? It's not any one thing,\n",
      "it's a lot of things. So what I'd like to do is\n",
      "start off sharing with you how I view the\n",
      "technology landscape, and this will lead into\n",
      "the set of opportunities. So lot of hype, lot of\n",
      "excitement about AI. And I think, a good\n",
      "way to think about AI is as a collection of tools. So this includes, a technique\n",
      "called supervised learning, which is very good at\n",
      "recognizing things or labeling things, and generative AI, which\n",
      "is a relatively new, exciting development. If you're familiar with AI, you\n",
      "may have heard of other tools. But I'm going to talk less\n",
      "about these additional tools, and I'll focus today on\n",
      "what I think are, currently, the two most important tools,\n",
      "which are supervised learning and generative AI. So supervised learning is\n",
      "very good at labeling things or very good at computing\n",
      "input to outputs or A to B mappings, given an input\n",
      "A, give me an output. For Example, given an\n",
      "email, we can use supervised learning to label it\n",
      "as spam or not spam. The most lucrative\n",
      "application of this that I've ever worked on is\n",
      "probably online advertising, where given an ad, we\n",
      "can label if a user likely to click on\n",
      "it, and therefore, show more relevant ads. For self-driving cars, given\n",
      "the sensor readings of a car, we can label it with\n",
      "where are the other cars. One project, that my\n",
      "team, AI Fund, worked on was ship route optimization. Where given a route the ship is\n",
      "taking or considering taking, we can label that\n",
      "with how much fuel we think this will consume,\n",
      "and use this to make ships more fuel efficient. Did a lot of work in automated\n",
      "visual inspection in factories. So you can take a picture\n",
      "of a smartphone, that was just manufactured and\n",
      "label, is there a scratch or any other defect in it. Or if you want to build a\n",
      "restaurant review, reputation monitoring system, you can\n",
      "have a little piece of software that looks at online\n",
      "restaurant reviews, and labels that as positive\n",
      "or negative sentiment. So one nice thing, one cool\n",
      "thing about supervised learning is that it's not useful for\n",
      "one thing, it's useful for all of these different applications,\n",
      "and many more, besides. Let me just walk\n",
      "through, concretely, the workflow one example\n",
      "of a supervised learning, labeling things kind of project. If you want to build a system\n",
      "to label restaurant reviews, you then collect a few data\n",
      "points or collect a data set. Where it say, the\n",
      "pastrami sandwich great, say that is positive. Servers are slow,\n",
      "that's negative. My favorite chicken\n",
      "curry, that's positive. And here, I've shown\n",
      "three data points, but you are building\n",
      "this, you may get thousands of\n",
      "data points like this or thousands of training\n",
      "examples, we call it. And the workflow of a machine\n",
      "learning project, of an AI project is, you get\n",
      "labeled data, maybe thousands of data points. Then you have an AI\n",
      "entry team train an AI model to learn from this data. And then finally,\n",
      "you would find, maybe a cloud service to\n",
      "run the trained AI model. And then you can feed it,\n",
      "best bubble tea I've ever had, and that's positive sentiment. And so, I think the\n",
      "last decade was maybe the decade of large scale\n",
      "supervised learning. What we found, starting\n",
      "about 10, 15 years ago was if you were to\n",
      "train a small AI model, so train a small neural\n",
      "network or small deep learning algorithm, basically,\n",
      "a small AI model, maybe not on a very\n",
      "powerful computer, then as you fed it more\n",
      "data, its performance would get better\n",
      "for a little bit but then it would flatten out. It would plateau,\n",
      "and it would stop being able to use the data\n",
      "to get better and better. But if you were to train a very\n",
      "large AI model, lots of compute on maybe powerful GPUs, then as\n",
      "we scaled up the amount of data we gave the machine\n",
      "learning model, its performance\n",
      "would kind of keep on getting better and better. So this is why when I started\n",
      "and led the Google Brain team, the primary mission that\n",
      "I directed the team to solve, at the time, was let's\n",
      "just build really, really large neural networks,\n",
      "that we then fed a lot of data to. And that recipe,\n",
      "fortunately, worked. And I think the idea of\n",
      "driving large compute and large scale of data, that\n",
      "recipe's really helped us, driven a lot of AI progress\n",
      "over the last decade. So if that was the\n",
      "last decade of AI, I think this decade\n",
      "is turning out to be also doing everything\n",
      "we had in supervised learning but adding to it the\n",
      "exciting tool of generative AI. So many of you,\n",
      "maybe all of you, have played with ChatGPT\n",
      "and Bard, and so on. But just given a piece of\n",
      "text, which you call a prompt, like I love eating, if you\n",
      "run this multiple times, maybe you get bagels cream\n",
      "cheese or my mother's meatloaf or out with friends,\n",
      "and the AI system can generate output like that. Given the amounts of buzz and\n",
      "excitement about generative AI, I thought I'd take just half\n",
      "a slide to say a little bit about how this works. So it turns out that generative\n",
      "AI, at least this type of text generation, the core of\n",
      "it is using supervised learning that inputs output\n",
      "mappings to repeatedly predict the next word. And so, if your system\n",
      "reads, on the internet, a sentence like, my favorite\n",
      "food is a bagel with cream cheese and lox, then this is\n",
      "translated into a few data points, where if it sees,\n",
      "my favorite food is A, in this case, try to guess that\n",
      "the right next word was bagel or my favorite food\n",
      "is a bagel, try to guess the next word\n",
      "is with, and similarly, if it sees that, in this\n",
      "case, the right guess for the next word\n",
      "would have been cream. So by taking texts that\n",
      "you find on the internet or other sources, and by using\n",
      "this input, output, supervised learning to try to repeatedly\n",
      "predict the next word, if you train a very large AI\n",
      "system on hundreds of billions of words, or in the case\n",
      "of the largest models, now more than a trillion words, then\n",
      "you get a large language model like ChatGPT. And there are additional, other\n",
      "important technical details. I talked about\n",
      "predicting the next word. Technically, these\n",
      "systems predict the next subword or part\n",
      "of a word called a token, and then there are other\n",
      "techniques like RLHF for further tuning the AI output\n",
      "to be more helpful, honest, and harmless. But at the heart of it\n",
      "is this using supervised learning to repeatedly\n",
      "predict the next word. That's really what's\n",
      "enabling the exciting, really fantastic progress on\n",
      "large language models. So while many people have\n",
      "seen large language models as a fantastic consumer tool. You can go to a website\n",
      "like ChatGPT's website or Bard's or other\n",
      "large language models and use it as a fantastic tool. There's one other trend, I\n",
      "think is still underappreciated, which is the power of\n",
      "large language models, not just as a consumer tool\n",
      "but as a developer tool. So it turns out that\n",
      "there are applications that used to take me months\n",
      "to build, that a lot of people can now build much faster by\n",
      "using a large language model. So specifically, the workflow\n",
      "for supervised learning, building the restaurant\n",
      "review system, say, would be that you need to\n",
      "get a bunch of labeled data, and maybe that takes a month, we\n",
      "get a few thousand data points. And then have an AI\n",
      "team train, and tune, and really get optimized\n",
      "performance on your AI model. Maybe that'll take three months. Then find a cloud\n",
      "service to run it. Make sure it's running robustly. Make sure it's\n",
      "recognized, maybe that'll take another three months. So pretty realistic timeline\n",
      "for building a commercial grade machine learning system\n",
      "is like 6 to 12 months. And so teams I've led, we often\n",
      "took roughly 6 to 12 months to build and deploy\n",
      "these systems. And some of them turned\n",
      "out to be really valuable. But this is a realistic timeline\n",
      "for building and deploying a commercial grade AI system. In contrast, with prompt-based\n",
      "AI, where you write a prompt. This is what the\n",
      "workflow looks like. You can specify a prompt, that\n",
      "takes maybe minutes or hours. And then, you can\n",
      "deploy it to the cloud, and that takes\n",
      "maybe hours or days. So there are now\n",
      "certain AI applications that used to take me,\n",
      "literally, six months, maybe a year to build, that\n",
      "many teams around the world can now build in maybe a week. And I think this is\n",
      "already starting, but the best is\n",
      "still yet to come. This is starting to open\n",
      "up a flood of a lot more AI applications that can be\n",
      "built by a lot of people. So I think many people\n",
      "still underestimate the magnitude of the flood\n",
      "of custom AI applications that I think is going\n",
      "to come down the pipe. Now, I know you\n",
      "probably were not expecting me to write\n",
      "code in this presentation, but that's what I'm going to do. So it turns out,\n",
      "this is all the code that I need in order to\n",
      "write a sentiment classifier. So I'm going to-- some of you will\n",
      "know Python, I guess. Import some tools\n",
      "from OpenAI, and then add this prompt, that says,\n",
      "classify the text below delimited by three\n",
      "dashes as having either a positive or\n",
      "negative sentiment. [INAUDIBLE],, I had a fantastic\n",
      "time at Stanford GSB. Learnt a lot, and also\n",
      "made great new friends. All right. So that's my prompt. And then I'm just\n",
      "going to run it. And I've never run it before. So I really hope-- thank goodness, it\n",
      "got the right answer. [APPLAUSE] And this is literally\n",
      "all the code it takes to build a\n",
      "sentiment classifier. And so, today, developers\n",
      "around the world can take, literally,\n",
      "maybe 10 minutes to build a system like this. And that's a very\n",
      "exciting development. So one of the things\n",
      "I've been working on was trying to teach\n",
      "online classes about how to use prompting, not\n",
      "just as a consumer tool but as a developer too. So just talking about\n",
      "the technology landscape, let me now share my\n",
      "thoughts on what are some of the AI opportunities I see. This shows what I think is\n",
      "the value of different AI technologies today, and\n",
      "I'll talk about three years from now. But the vast majority of\n",
      "financial value from AI today is, I think,\n",
      "supervised learning, where for a single\n",
      "company like Google can be worth more than\n",
      "$100 billion US a year. And also, there are\n",
      "millions of developers building supervised\n",
      "learning applications. So it's already massively\n",
      "valuable, and also with tremendous momentum\n",
      "behind it just because of the sheer effort in\n",
      "finding applications and building applications. And then, generative AI is the\n",
      "really exciting new entrant, which is much smaller right now. And then, there\n",
      "are the other tools that I'm including\n",
      "for completeness. If the size of these circles\n",
      "represent the value today, this is what I think it\n",
      "might grow to in three years. So supervised learning,\n",
      "already really massive, may double, say,\n",
      "in the next three years, from truly massive\n",
      "to even more massive. And generative AI, which is\n",
      "much smaller today, I think, will much more than double in\n",
      "the next three years because of the number-- the amount\n",
      "of developer interest, the amount of venture\n",
      "capital investments, the number of large corporates\n",
      "exploring applications. And I also just\n",
      "want to point out, three years is a very\n",
      "short time horizon. If it continues to compound\n",
      "in anything near this rate, then in six years, it will\n",
      "be even vastly larger. But this light shaded\n",
      "region in green or orange, that light shaded region\n",
      "is where the opportunity is for either new startups or for\n",
      "large companies, incumbents, to create and to\n",
      "enjoy value capture. But one thing I hope you\n",
      "take away from this slide is that all of\n",
      "these technologies are general purpose\n",
      "technologies. So in the case of\n",
      "supervised learning, a lot of the work that had to\n",
      "be done over the last decade, but is continuing\n",
      "for the next decade, is to identify and to execute\n",
      "on the concrete use cases. And that process is also\n",
      "kicking off for generative AI. So for this part of\n",
      "the presentation, I hope you take away from it\n",
      "that general purpose technology is a useful for many\n",
      "different tasks, lot of value remains to be created\n",
      "using supervised learning. And even though, we're nowhere\n",
      "near finishing figuring out the exciting use cases\n",
      "of supervised learning, we have this other fantastic\n",
      "tool of generative AI, which further expands the set of\n",
      "things we can now do using AI. But one caveat, which\n",
      "is that there will be short term fads along the way. So I don't know if\n",
      "some of you might remember the app called Lensa. This is the app\n",
      "that will let you upload pictures of\n",
      "yourself, and then will render a cool\n",
      "picture of you as an astronaut or a\n",
      "scientist or something. And it was a good idea\n",
      "and people liked it. And its revenue just took\n",
      "off like crazy like that, through last December. And then it did that. And that's because Lensa\n",
      "was-- it was a good idea. People liked it. But it was a relatively\n",
      "thin software layer on top of someone else's\n",
      "really powerful APIs. And so even though it\n",
      "was a useful product, it was in a defensible business. And when I think\n",
      "about apps like Lensa, I'm actually reminded of when\n",
      "Steve Jobs gave us the iPhone. Shortly after,\n",
      "someone wrote an app that I paid $1.99 for, to\n",
      "do this, to turn on the LED, to turn the phone\n",
      "into a flashlight. And that was also a good\n",
      "idea to write an app to turn on the LED\n",
      "light, but it also wasn't a defensible long term-- also didn't create\n",
      "very long term value because it was easily\n",
      "replicated, and underpriced, and eventually\n",
      "incorporated into iOS. But with the rise of iOS,\n",
      "with the rise of iPhone, someone also figured out how\n",
      "to build things like Uber, and Airbnb, and Tinder. The very long term, very\n",
      "defensible businesses that created sustaining value. And I think, with the\n",
      "rise of generative AI or the rise of new AI\n",
      "tools, I think, really, what excites me\n",
      "is the opportunity to create those really deep,\n",
      "really hard applications that hopefully can create\n",
      "very long term value. So the first trend\n",
      "I want to share is AI is a general\n",
      "purpose technology. And a lot of the work\n",
      "that lies ahead of us, is to find the very diverse\n",
      "use cases and to build them. There's a second\n",
      "trend I want to share with you, which relates to why\n",
      "AI isn't more widely adopted yet. It feels like a bunch of us\n",
      "have been talking about AI for 15 years or something. But if you look at where\n",
      "the value of AI is today, a lot of it is still very\n",
      "concentrated in consumer software internet. Once you got outside tech or\n",
      "consumer software internet, there's some AI adoption\n",
      "but it all feels very early. So why is that? It turns out, if\n",
      "you were to take all current and\n",
      "potential AI projects, and sort them in\n",
      "decreasing order of value, then to the left of this curve,\n",
      "of the head of this curve, are the multi-billion dollar\n",
      "projects like advertising or web search or for e-commerce\n",
      "product recommendations or company like Amazon. And it turns out that\n",
      "about 10, 15 years ago, [? there's ?] my\n",
      "friends and I, we figured out a recipe\n",
      "for how to hire, say, 100 engineers to write\n",
      "one piece of software to serve more relevant\n",
      "ads, and apply that one piece of software\n",
      "to a billion users, and generate massive\n",
      "financial value. So that works. But once you go outside\n",
      "consumer software internet, hardly anyone has 100\n",
      "million or a billion users that you can write and apply\n",
      "one piece of software to. So once you go to\n",
      "other industries, as we go from the head\n",
      "of this curve on the left over to the long tail, these\n",
      "are some of the projects I see, and I'm excited about. I was working with\n",
      "a pizza maker that was taking pictures of the pizza\n",
      "they were making because they needed to do things like make\n",
      "sure that the cheese is spread evenly. So this is about a\n",
      "$5 million project. But that recipe of hiring a\n",
      "hundred engineers or dozens of engineers to\n",
      "work on a $5 million project, that\n",
      "doesn't make sense. Or there's another\n",
      "great example. Working with a\n",
      "agriculture company that with them, we figured\n",
      "out that if we use cameras to find out how\n",
      "tall is the wheat, and wheat is often\n",
      "bent over because of wind or rain or\n",
      "something, and we can chop off the wheat\n",
      "at the right height, then that results in more\n",
      "food for the farmer to sell, and is also better\n",
      "for the environment. But this is another\n",
      "$5 million project, that that old recipe of\n",
      "hiring a large group of highly skilled engineers to work\n",
      "on this one project, that doesn't make sense. And similarly materials\n",
      "grading, cloth grading, sheet metal grading,\n",
      "many projects like this. So whereas to the left,\n",
      "in the head of this curve, there's a small\n",
      "number of, let's say, multi-billion dollar\n",
      "projects, and we know how to execute\n",
      "those delivering value. In other industries, I'm\n",
      "seeing a very long tail of tens of thousands,\n",
      "of let's call them, $5 million projects,\n",
      "that until now, have been very\n",
      "difficult to execute on because of the high\n",
      "cost of customization. The trend that I\n",
      "think is exciting is that the AI community has\n",
      "been building better tools that lets us aggregate\n",
      "these use cases, and make it easy for the end\n",
      "user to do the customization. So specifically,\n",
      "I'm seeing a lot of exciting low code\n",
      "and no code tools, that enable the user to\n",
      "customize the AI system. What this means\n",
      "is instead of me, needing to worry that much\n",
      "about pictures of pizza, we have tools-- we're starting to see\n",
      "tools that can enable the IT department of\n",
      "the pizza making factory to train AI system on\n",
      "their own pictures of pizza to realize this $5\n",
      "million worth of value. And by the way, the\n",
      "pictures of pizza, they don't exist\n",
      "on the internet. So Google and Bing don't have\n",
      "access to these pictures, we need tools that can\n",
      "be used by, really, the pizza factory themselves,\n",
      "to build, and deploy, and maintain their own\n",
      "custom AI system that works on their own pictures of pizza. And broadly, the technology\n",
      "for enabling this, some of it is prompting, text\n",
      "prompting, visual prompting, but really, large language\n",
      "models and similar tools like that or a technology\n",
      "called data-centric AI, whereby, instead of asking the pizza\n",
      "factory to write a lot of code, which is challenging, we can\n",
      "ask them to provide data which turns out to be more feasible. And I think the second\n",
      "trend is important, because I think this is a key\n",
      "part of the recipe for taking the value of AI, which\n",
      "so far still feels very concentrated in the tech\n",
      "world and the consumer software internet world, and pushing this\n",
      "out to all industries, really to the rest of the\n",
      "economy, which-- sometimes it's easy to forget,\n",
      "the rest of the economy is much bigger than\n",
      "the tech world. So the two trends I shared,\n",
      "AI as a general purpose technology, lots of\n",
      "concrete use cases to be realized as well as low\n",
      "code, no code, easy to use tools, enabling AI to be\n",
      "deployed in more industries. How do we go after\n",
      "these opportunities? So about five years ago, there\n",
      "was a puzzle I wanted to solve, which is-- I felt that many valuable AI\n",
      "projects are now possible. And I was thinking, how\n",
      "do we get them done? And having led teams in\n",
      "Google, and Baidu, in big tech companies, I had a\n",
      "hard time figuring out how I could operate a\n",
      "team in a big tech company to go after a very diverse set\n",
      "of opportunities in everything from maritime shipping to\n",
      "education to financial services to healthcare, and on and on. It's just very diverse\n",
      "use cases, very diverse go to markets, and very\n",
      "diverse customer bases and applications. And I felt that the\n",
      "most efficient way to do this would\n",
      "be if we can start a lot of different companies\n",
      "to pursue these very diverse opportunities. So that's why I ended up\n",
      "starting AI Fund, which is a venture studio\n",
      "that builds startups to pursue a diverse set\n",
      "of AI opportunities. And, of course, in addition\n",
      "to lots of startups, incumbent companies\n",
      "also have a lot of opportunities to integrate\n",
      "AI into existing businesses. In fact, one pattern I'm seeing\n",
      "for incumbent businesses is distribution is often one of\n",
      "the significant advantages of incumbent companies, if\n",
      "they play their cards right, can allow them to integrate\n",
      "AI into their products, quite efficiently. But just to be concrete,\n",
      "where are the opportunities? So I think of this as-- this is\n",
      "what I think of as an AI stack. At the bottom level is the\n",
      "hardware, semiconductor layer. Fantastic opportunities there,\n",
      "but very capital intensive, very concentrated. So needs a lot of resources,\n",
      "relatively few winners. So some people can\n",
      "and should play there. I personally don't like\n",
      "to play there myself. There's also the\n",
      "infrastructure layer. Also fantastic opportunities,\n",
      "but very capital intensive, very concentrated. So I tend not to play\n",
      "there myself, either. And then there's the\n",
      "developer tool layer. What I showed you just now was-- I was actually using OpenAI's\n",
      "API as a developer tool. And then, I think\n",
      "the developer tool sector is a hypercompetitive. Look at all the startups\n",
      "chasing OpenAI right now. But there will be\n",
      "some mega winners. And so I sometimes play\n",
      "here, but primarily, when I think of a meaningful\n",
      "technology advantage, because I think that\n",
      "earns you the right or earns you a better shot at\n",
      "being one of the mega winners. And then lastly, even though\n",
      "a lot of the media attention and the buzz is in the\n",
      "infrastructure and developer tooling layer, it turns out\n",
      "that layer can be successful only if the application layer\n",
      "is even more successful. And we saw this with the\n",
      "rise of SaaS as well. Lot of the buzz and excitement\n",
      "is on the technology, the tooling layer. Which is fine. Nothing wrong with that. But the only way for\n",
      "that to be successful is if the application layer\n",
      "is even more successful, so that, frankly,\n",
      "they can generate enough revenue to pay the\n",
      "infrastructure, and the tooling layer. So, actually, let me\n",
      "mention one example. Amorai-- I was actually just\n",
      "texting the CEO yesterday. But Amorai is a\n",
      "company that we built that uses AI for romantic\n",
      "relationship coaching. And just to point\n",
      "out, I'm an AI guy. And I feel like I know\n",
      "nothing really about romance. And if you don't believe\n",
      "me, you can ask my wife, she will confirm that I\n",
      "know nothing about romance. But when we went\n",
      "to build this, we wanted to get together with the\n",
      "former CEO of Tinder, Renate Nyborg. And with my team's\n",
      "expertise in AI, and her expertise\n",
      "in relationships because she ran Tinder, she\n",
      "knows more about relationships than I think anyone\n",
      "I know, we're able to build something\n",
      "pretty unique using AI for kind of romantic\n",
      "relationship mentoring. And the interesting thing\n",
      "about applications like these is when we look around,\n",
      "how many teams in the world are simultaneously expert\n",
      "in AI and in relationships? And so at the\n",
      "application layer, I'm seeing a lot of\n",
      "exciting opportunities that seem to have a\n",
      "very large market, but where the\n",
      "competition sets is very light, relative to the\n",
      "magnitude of the opportunity. It's not that there\n",
      "are no competitors, but it's just much less intense\n",
      "compared to the developer tool or the infrastructure layers. And so, because I've spent\n",
      "a lot of time iterating on a process of building\n",
      "startups, what I'm going to do is just, very\n",
      "transparently, tell you the recipe we've developed\n",
      "for building startups. And so after many years of\n",
      "iteration and improvement, this is how we now\n",
      "build startups. My team's always had access\n",
      "to a lot of different ideas, internally generated,\n",
      "ideas from partners. And I want to walk through this\n",
      "with one example of something we did, which is a\n",
      "company Bearing AI, which uses AI to make\n",
      "ships more fuel efficient. So this idea came to me\n",
      "when, a few years ago, a large Japanese\n",
      "conglomerate called Mitsui, that is a major shareholder and\n",
      "operates major shipping lines, they came to me and they\n",
      "said, hey, Andrew, you should build a business to\n",
      "use AI to make ships more fuel efficient. And the specific\n",
      "idea was, think of it as a Google Maps for ships. We can suggest a ship or\n",
      "tell a ship how to steer, so that you still get to\n",
      "your destination on time, but using, it turns out,\n",
      "about 10% less fuel. And so what we now do is\n",
      "we spend about a month, validating the idea. So double check, is this idea\n",
      "even technically feasible, and then talk to\n",
      "prospective customers to make sure there\n",
      "is a market need. So we spent up to about\n",
      "a month doing that. And if it passes\n",
      "this stage, then we will go and recruit a CEO to\n",
      "work with us on the project. When I was starting,\n",
      "out I used to spend a long time working\n",
      "on a project myself, before bringing on a CEO. But after iterating, we\n",
      "realized that bringing on a leader at\n",
      "the very beginning to work with us, it reduces\n",
      "a lot of the burden of having to transfer knowledge\n",
      "or having a CEO come in and having to revalidate\n",
      "what [? we ?] discovered. So the process is, we've,\n",
      "learned much more efficient, we just bring the leader\n",
      "at the very start. And so in the case\n",
      "of Bearing AI, we found a fantastic\n",
      "CEO, Dylan Keil, who is a reputed entrepreneur,\n",
      "one successful exit before. And then we spent three\n",
      "months, six, two week sprints, to work with them\n",
      "to build a prototype as well as do deep\n",
      "customer validation. If it survives\n",
      "this stage, and we have about a two thirds,\n",
      "66% survival rate, we then write the\n",
      "first check in, which then gives the\n",
      "company resources to hire an executive\n",
      "team, build the key team, get an MVP working, minimum\n",
      "viable product working, and get some real customers. And then after that,\n",
      "hopefully, then successfully raises additional external\n",
      "rounds of funding, and can keep on\n",
      "growing and scaling. So I'm really proud of\n",
      "the work that my team was able to do to support\n",
      "Mitsui's idea, and Dylan Keil, as CEO. And today, there are hundreds\n",
      "of ships, on the high seas right now, that are steering\n",
      "themselves differently because of Bearing AI. And 10% fuel savings\n",
      "translates to around to maybe $450,000 in savings in\n",
      "fuel, per, ship per year. And, of course, it's also,\n",
      "frankly, quite a bit better for the environment. And I think this\n",
      "startup, I think, would not have existed if not\n",
      "for Dylan's fantastic work, and then also, Mitsui\n",
      "bringing this idea to me. And I like this example because\n",
      "this is another one is like-- this is a startup idea\n",
      "that, just to point out, I would never have\n",
      "come up with myself. Because I've been\n",
      "on a boat but what do I know about\n",
      "maritime shipping. But is the deep subject\n",
      "matter expertise of Mitsui, that had this insight,\n",
      "together with Dylan, and then my team's expertise\n",
      "in AI, that made this possible. And so as I operate in\n",
      "AI, one thing I've learned is my swim lane is\n",
      "AI, and that's it. Because I don't have time or\n",
      "it's very difficult for me to be expert in\n",
      "maritime shipping, and romantic relationships,\n",
      "and health care, and financial services,\n",
      "and on, and on, and on. And so I've learned\n",
      "that if I can just help get a accurate\n",
      "technical validation, and then use AI\n",
      "resources to make sure the AI tech is built\n",
      "quickly and well, and I think, we've\n",
      "always managed to help the companies build\n",
      "a strong technical team, then partnering with subject\n",
      "matter experts often results in exciting new opportunities. And I want to share with you\n",
      "one other weird aspect of-- one other weird\n",
      "thing I've learned about building\n",
      "startups, which is I like to engage only when\n",
      "there's a concrete idea. And this runs counter to\n",
      "a lot of the advice you hear from the design thinking\n",
      "methodology, which often says, don't rush to solutioning. Explore a lot of alternatives\n",
      "before you do a solution. Honestly, we tried\n",
      "that, it was very slow. But what we've learned\n",
      "is that at the ideation stage, if someone comes to\n",
      "me and says, hey, Andrew, you should apply AI\n",
      "to financial services. Because I'm not a subject matter\n",
      "expert in financial services, it's very slow for\n",
      "me to go and learn enough about financial services,\n",
      "to figure out what to do. I mean, eventually, you\n",
      "could get to a good outcome, but it's a very labor\n",
      "intensive, very slow, very expensive process, for\n",
      "me, to try to learn industry after industry. In contrast, one of my\n",
      "partners wrote this idea as a tongue in cheek,\n",
      "not really seriously. But, let's say,\n",
      "[INAUDIBLE] by GPT, let's eliminate commercials\n",
      "by automatically buying every product advertised\n",
      "in exchange for not having to see any ads, it's\n",
      "not a good idea, but it is a concrete idea. And it turns out, concrete ideas\n",
      "can be validated or falsified, efficiently. They also give a team a\n",
      "clear direction to execute. And I've learned that\n",
      "in today's world, especially, with the excitement,\n",
      "the buzz, the exposure to AI of a lot of people, it\n",
      "turns out that there are a lot of subject matter\n",
      "experts in today's world, that have deeply thought about\n",
      "a problem for months, sometimes even one or two years. But they've not yet\n",
      "had a build partner. And when we get together\n",
      "with them, and hear, and they share\n",
      "the idea of us, it allows us to work with\n",
      "them to very quickly go into validation and building. And I find that this\n",
      "works because there are a lot of people that\n",
      "have already done the design thinking thing of exploring\n",
      "a lot of ideas and winnowing down to really good ideas. And there are-- I\n",
      "find that there are so many good ideas\n",
      "sitting out there, that no one is working on. That finding those good ideas\n",
      "that someone has already had, and wants to share with us,\n",
      "and wants to build partner for, that turns out to be a\n",
      "much more efficient engine. So before I wrap up, we'll go\n",
      "to the question in a second, just a few slides to talk\n",
      "about risk and social impact. So AI is very\n",
      "powerful technology. To say something you'd\n",
      "probably guess, my teams and I, we only work on projects\n",
      "that move humanity forward. And we have multiple\n",
      "times killed projects that we assess to be\n",
      "financially sound, based on ethical grounds. It turns out, I've\n",
      "been surprised and sometimes dismayed at\n",
      "the creativity of people to come up with good ideas. So to come up with\n",
      "really bad ideas that seem profitable but really\n",
      "should not be built. We've killed a few\n",
      "projects on those grounds. And then, I think, has to be\n",
      "acknowledged that AI today does have problems with\n",
      "bias, fairness, and accuracy. But also the technology\n",
      "is improving quickly. So I see that AI\n",
      "systems today are less biased than six\n",
      "months ago, and more fair than six months\n",
      "ago, which is not to dismiss the importance\n",
      "of these problems. They are problems and we should\n",
      "continue to work on them. But I'm also gratified\n",
      "at the number of teams working\n",
      "hard on these issues to make them much better. When I think of the\n",
      "biggest risks of AI. I think that the biggest risks-- one of the biggest risks\n",
      "is the disruption to jobs. This is a diagram from a paper\n",
      "by our friend at the University of Pennsylvania, and\n",
      "some folks at OpenAI, analyzing the exposure\n",
      "of different jobs to AI automation. And it turns out that, whereas,\n",
      "the previous wave of automation mainly-- the most exposed jobs were\n",
      "often the lower wage jobs, such as when we put\n",
      "robots into factories. With this current\n",
      "wave of automation, is actually the higher\n",
      "wage jobs, further, to the right of this\n",
      "axis, that seems to have more of their tasks\n",
      "exposed to automation. So even as we create\n",
      "tremendous value using AI, I feel like, as citizens,\n",
      "and our corporations, and our governments,\n",
      "and, really, our society, I feel\n",
      "a strong obligation to make sure that people,\n",
      "especially people whose livelihoods are disrupted,\n",
      "are still well taken care of, are still treated well. And then lastly,\n",
      "there's also been-- it feels like every time there's\n",
      "a big wave of progress in AI, there's a big wave of hype about\n",
      "artificial general intelligence as well. When DeepLearning started\n",
      "work really well 10 years ago, there was a lot\n",
      "of hype about AGI. And now, the generative\n",
      "AI is working really well, there's another wave\n",
      "of hype about AGI. But I think that artificial\n",
      "general intelligence, AI that can do\n",
      "anything a human can do is still decades away, maybe 30\n",
      "to 50 years, maybe even longer. I hope we'll see it\n",
      "in our lifetimes. But I don't think\n",
      "there's any time soon. One of the challenges is\n",
      "that the biological path to intelligence, like\n",
      "humans and the digital path to intelligence, AI, they've\n",
      "taken very different paths. And the funny thing about\n",
      "the definition of AGI is you're benchmarking this\n",
      "very different digital path to intelligence with\n",
      "really the biological path to intelligence. So I think, large language\n",
      "models are smarter than any of us in\n",
      "certain key dimensions, but much dumber than any\n",
      "of us in other dimensions. And so forcing it to do\n",
      "everything a human can do is like a funny comparison. But I hope we'll get there. Hopefully, within our lifetimes. And then there's also\n",
      "a lot of, I think, overblown hype about AI creating\n",
      "extinction risks for humanity. Candidly, I don't see it. I just don't see how AI creates\n",
      "any meaningful extinction risk for humanity. I think that people worry\n",
      "we can't control AI. But we have lots of, AI will be\n",
      "more powerful than any person. But with lots of experience,\n",
      "steering, very powerful entities, such as corporations\n",
      "or nation states that are far more powerful\n",
      "than any single person, and making sure they, for the\n",
      "most part, benefit humanity. And also technology\n",
      "develops gradually. The so-called hot\n",
      "take off scenario, where it's not\n",
      "really working today, and then suddenly,\n",
      "one day, overnight, it works brilliantly, and we\n",
      "achieve super intelligence, takes over the world. That's just not realistic. And I think the AI technology\n",
      "will develop slowly, like all the-- and then it gives\n",
      "us plenty of time to make sure that we\n",
      "provide oversight and can manage it to be safe. And lastly, if you look at\n",
      "the real extinction risk to humanity, such\n",
      "as, fingers crossed, the next pandemic\n",
      "or climate change, leading to a massive\n",
      "de-population of some parts of the planet, or much lower\n",
      "odds, but maybe someday, an asteroid doing to us what\n",
      "it had done to the dinosaurs. I think if we look at the\n",
      "actual real extinction risk to humanity, AI\n",
      "having more intelligence, even artificial\n",
      "intelligence in the world, would be a key part\n",
      "of the solution. So I feel like if you want\n",
      "humanity to survive and thrive for the next 1,000 years,\n",
      "rather than slowing AI down, which some people\n",
      "propose, I would rather make AI go as fast as possible. So with that, just to summarize,\n",
      "this is my last slide. I think that AI, as a\n",
      "general purpose technology creates a lot of new\n",
      "opportunities for everyone. And a lot of the exciting\n",
      "and important work that lies ahead of us all is to go\n",
      "and build those concrete use cases, and hopefully,\n",
      "in the future, hopefully, I'll have\n",
      "opportunities to maybe engage with more of you on\n",
      "those opportunities as well. So with that, let me just\n",
      "say, thank you all very much. [APPLAUSE]\n"
     ]
    }
   ],
   "source": [
    "print(f\"Łacznie w książka jest {len(clips)} transkryptów stron.\")\n",
    "print()\n",
    "print(f\"Pierwszy transkrypt w kolekcji to:\\n{clips[0].page_content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb806455-7fc5-46bc-8e50-e1b3b4f31f82",
   "metadata": {},
   "source": [
    "Na koniec rozszerzemy tekst z książek o transkrypty z filmów, żeby mieć jedną wspólną listę kontekstów."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bf20590d-c312-4276-b0f7-13c823c62140",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-08T07:47:43.661751700Z",
     "start_time": "2024-01-08T07:47:43.643676300Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "161\n"
     ]
    }
   ],
   "source": [
    "docs.extend(clips)\n",
    "print(len(docs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dab6594a-f19e-4150-97a3-3ed383c07787",
   "metadata": {},
   "source": [
    "## Dzielenie tekstu na części\n",
    "\n",
    "Na temat dzielenia tekstu na mniejsze fragmenty można by przygotować osobny tutorial. Można dzielić na podstawie znaków, tokenów, parsować zdania za pomocą nltk, wykrywać akapity i rozdziały, tworzyć hierarchie fragmentów. Przykłady bardziej zaawansowanych technik z wykorzystaniem LlamaIndex można znaleźć na [blogach](https://blog.llamaindex.ai/a-cheat-sheet-and-some-recipes-for-building-advanced-rag-803a9d94c41b) i [darmowych kursach](https://www.deeplearning.ai/short-courses/building-evaluating-advanced-rag/).\n",
    "\n",
    "W ramach tych zajęć skorzystamy tylko z jednego prostego podejścia do dzielenia tekstu opartego na wybranych znakach. `RecursiveCharacterTextSplitter`. bo tak nazywa się klasa z której skorzystamy, stara się dzielić tekst na mniejsze części o zadanej długości. W tym celu wyszukuje wskazanych znaków i wybiera pierwszy, który pozwoli uzyskać fragment nie dłuższy niż wskazana liczba znaków. Zobacz jak to działa na przykładzie poniżej."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4d1c4409-fd37-4547-a017-3067188cc376",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-08T07:49:16.893220500Z",
     "start_time": "2024-01-08T07:49:14.470364500Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'song' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[11], line 8\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mlangchain\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mtext_splitter\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m RecursiveCharacterTextSplitter\n\u001B[0;32m      3\u001B[0m r_splitter \u001B[38;5;241m=\u001B[39m RecursiveCharacterTextSplitter(\n\u001B[0;32m      4\u001B[0m     chunk_size\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m100\u001B[39m,\n\u001B[0;32m      5\u001B[0m     chunk_overlap\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0\u001B[39m,\n\u001B[0;32m      6\u001B[0m     separators\u001B[38;5;241m=\u001B[39m[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124m. \u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m, \u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m \u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n\u001B[0;32m      7\u001B[0m )\n\u001B[1;32m----> 8\u001B[0m r_splitter\u001B[38;5;241m.\u001B[39msplit_text(song)\n",
      "\u001B[1;31mNameError\u001B[0m: name 'song' is not defined"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "r_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=100,\n",
    "    chunk_overlap=0,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \"\\. \", \", \", \" \", \"\"]\n",
    ")\n",
    "r_splitter.split_text(song)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b5e5164-61fd-45f5-bb94-aa3b6ae2efbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "r_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=20,\n",
    "    chunk_overlap=10,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \"\\. \", \", \", \" \", \"\"]\n",
    ")\n",
    "r_splitter.split_text(song)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7b33ec4-23e9-4184-a543-da256b8c5ed1",
   "metadata": {},
   "source": [
    "**Zad. 4: Podziel dokumenty w zmiennej `docs` na części o długości 750 z zakładką o rozmiarze 150. Możesz do tego użyć fukcji `split_documents` zamiast `split_text`. Wynik przypisz do zmiennej `splits`. Sprawdź ile fragmentów zawiera `splits` i porównaj to z liczbą elementów w `docs`.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eb6e65f4-6fde-40df-bc28-2cda3b8e3330",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-08T07:51:55.576746100Z",
     "start_time": "2024-01-08T07:51:55.534288500Z"
    }
   },
   "outputs": [],
   "source": [
    "r_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=750,\n",
    "    chunk_overlap=150,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \"\\. \", \", \", \" \", \"\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d115fea0-88fe-4371-bb7b-b016c53375e1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-08T07:52:06.468615500Z",
     "start_time": "2024-01-08T07:52:06.354980Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "601"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "splits = r_splitter.split_documents(docs)\n",
    "len(splits)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4be0515d-50ba-45b8-b5ae-668f7c17fbc1",
   "metadata": {},
   "source": [
    "## Tworzenie i przechowywanie zanurzeń"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52f0661c-bc88-4b23-bc44-acb8714ce9b9",
   "metadata": {},
   "source": [
    "Jak wiadomo z wykładów, tekst można zapisywać do baz w różnorakich formatach, jednak dominują formaty wektorowe. Pracując z LLMami będzie nam zależeć na wyszukiwaniu semantycznym, czyli opratnym na znaczeniu tekstu a nie na występowaniu konkretnych słów. Użyjemy zanurzeń od OpenAI, ale można równie dobrze korzystać z zanurzeń udostępnianych na HuggingFace (no. BAAI/bge-small-en-v1.5 albo BAAI/bge-large-en-v1.5) i liczyć je lokalnie na komputerze.\n",
    "\n",
    "Wykonaj poniższy kod, aby policzyć zanurzenia dla przykładów z wykładu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c0749277-1645-4f60-b04e-1e75197409fc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-08T07:58:50.694801Z",
     "start_time": "2024-01-08T07:58:49.984899700Z"
    }
   },
   "outputs": [
    {
     "ename": "ValidationError",
     "evalue": "1 validation error for OpenAIEmbeddings\n__root__\n  Did not find openai_api_key, please add an environment variable `OPENAI_API_KEY` which contains it, or pass `openai_api_key` as a named parameter. (type=value_error)",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValidationError\u001B[0m                           Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[15], line 2\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mlangchain_openai\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m OpenAIEmbeddings\n\u001B[1;32m----> 2\u001B[0m embedding \u001B[38;5;241m=\u001B[39m OpenAIEmbeddings()\n\u001B[0;32m      4\u001B[0m sentence1 \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mJa lubię eksplorację danych.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m      5\u001B[0m sentence2 \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mJa lubię pływać.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\zed\\Lib\\site-packages\\pydantic\\v1\\main.py:341\u001B[0m, in \u001B[0;36mBaseModel.__init__\u001B[1;34m(__pydantic_self__, **data)\u001B[0m\n\u001B[0;32m    339\u001B[0m values, fields_set, validation_error \u001B[38;5;241m=\u001B[39m validate_model(__pydantic_self__\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__class__\u001B[39m, data)\n\u001B[0;32m    340\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m validation_error:\n\u001B[1;32m--> 341\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m validation_error\n\u001B[0;32m    342\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m    343\u001B[0m     object_setattr(__pydantic_self__, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m__dict__\u001B[39m\u001B[38;5;124m'\u001B[39m, values)\n",
      "\u001B[1;31mValidationError\u001B[0m: 1 validation error for OpenAIEmbeddings\n__root__\n  Did not find openai_api_key, please add an environment variable `OPENAI_API_KEY` which contains it, or pass `openai_api_key` as a named parameter. (type=value_error)"
     ]
    }
   ],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "embedding = OpenAIEmbeddings()\n",
    "\n",
    "sentence1 = \"Ja lubię eksplorację danych.\"\n",
    "sentence2 = \"Ja lubię pływać.\"\n",
    "sentence3 = \"Ja uwielbiam biegać.\"\n",
    "\n",
    "embedding1 = embedding.embed_query(sentence1)\n",
    "embedding2 = embedding.embed_query(sentence2)\n",
    "embedding3 = embedding.embed_query(sentence3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "631eb324-eb7c-40e3-8444-4b3a1f48db35",
   "metadata": {},
   "source": [
    "**Zad. 5: Podejrzyj jak wygląda takie zanurzenie i jaką ma długość. Następnie policz odległość między każdą parą zanurzeń. Czy zanurzenia 2 i 3 są do siebie bardziej podobne niż pozostałe pary?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c102fe40-2e67-4572-beaf-665d73ac8c5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(embedding2))\n",
    "\n",
    "print(np.dot(embedding1, embedding2))\n",
    "print(np.dot(embedding1, embedding3))\n",
    "print(np.dot(embedding2, embedding3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1ebb65d-6457-499e-becd-ab3ef2d14b81",
   "metadata": {},
   "source": [
    "Gdy już wiemy jak działa zanurzenia i mamy odpowiedni obiekt do tworzenia takowych w zmiennej `embedding`, czas stworzyć bazę danych dla naszych tekstów. Opcji jest wiele, ale my skorzystamy z bazy Chroma, która potrafi działać w pamięci jak i szybko stworzyć małą bazę sqlite lokalnie na dysku.\n",
    "\n",
    "Uruchom poniższy kod, aby stworzyć bazę zanurzeń, zapisać ją na dysk i zobaczyć ile elementów ma w środku."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3d411544-863c-4c04-8463-9ddaea4cd667",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-08T08:02:13.605087Z",
     "start_time": "2024-01-08T08:02:13.533224Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'embedding' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[16], line 8\u001B[0m\n\u001B[0;32m      3\u001B[0m \u001B[38;5;66;03m#!rm -rf ./chroma\u001B[39;00m\n\u001B[0;32m      4\u001B[0m persist_directory \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m./chroma/\u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[0;32m      6\u001B[0m vectordb \u001B[38;5;241m=\u001B[39m Chroma\u001B[38;5;241m.\u001B[39mfrom_documents(\n\u001B[0;32m      7\u001B[0m     documents\u001B[38;5;241m=\u001B[39msplits,\n\u001B[1;32m----> 8\u001B[0m     embedding\u001B[38;5;241m=\u001B[39membedding,\n\u001B[0;32m      9\u001B[0m     persist_directory\u001B[38;5;241m=\u001B[39mpersist_directory\n\u001B[0;32m     10\u001B[0m )\n\u001B[0;32m     11\u001B[0m vectordb\u001B[38;5;241m.\u001B[39mpersist()\n\u001B[0;32m     12\u001B[0m \u001B[38;5;28mprint\u001B[39m(vectordb\u001B[38;5;241m.\u001B[39m_collection\u001B[38;5;241m.\u001B[39mcount())\n",
      "\u001B[1;31mNameError\u001B[0m: name 'embedding' is not defined"
     ]
    }
   ],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "#!rm -rf ./chroma\n",
    "persist_directory = './chroma/'\n",
    "\n",
    "vectordb = Chroma.from_documents(\n",
    "    documents=splits,\n",
    "    embedding=embedding,\n",
    "    persist_directory=persist_directory\n",
    ")\n",
    "vectordb.persist()\n",
    "print(vectordb._collection.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53b66f9f-a429-4d43-822d-e8d03a3534d7",
   "metadata": {},
   "source": [
    "Po stworzeniu bazy możemy wypróbować wspomniane wyszukiwanie semantyczne. Poniżej napiszemy treść przykładowego zapytania i poprosimy o 3 najbardziej pasujące fragmenty z bazy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc4e99e4-a75d-4bee-81fd-cd8d0d982508",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"What is an eyeball dataset?\"\n",
    "relevant_splits = vectordb.similarity_search(question, k=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76859aa7-6fc1-4e37-9f97-f0f4a360ebe5",
   "metadata": {},
   "source": [
    "**Zad. 6: Sprawdź ile fragmentów zwróciła baza. Co jest w tych fragmentach? Z których książek lub filmów one pochodzą?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3ff0dfb-e3c5-4172-94e6-04c0feafa6bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "32780ee7-7888-471d-aff7-6c5912186e96",
   "metadata": {},
   "source": [
    "Powyżej zapytanie semantyczne zwróciło obiecujące wyniki. Czasami niestety to nie wystarcza. W przypadku duplikatów w bazie, trzeba dbać o różnorodność zwracanych fragmentów. W innych przypadkach trzeba oprócz wyszukiwania semantycznego ograniczyć się do wybranych dokumentów bo są one wprost wskazane w pytaniu. Tymi rzeczami nie mamy czasu się zajmować, ale w zależności od potrzeb można takie problemy rozwiązywać odrobiną technik z tradycyjnych baz danych lub poprosić LLM o pomoc przy zapytaniu do bazy kontekstów."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f49fda04-5cab-4a77-8fd2-261f05466c45",
   "metadata": {},
   "source": [
    "## Odpowiadanie na pytania z wykorzystaniem kontekstu (RAG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f760051-25b2-4db9-9e67-491fca2d4294",
   "metadata": {},
   "source": [
    "Po zebraniu dokumentów, podzieleniu ich na mniejsze fragmenty, policzeniu zanurzeń i zapisaniu ich do bazy, możemy przejść do RAG. To jest moment w, którym błyszczy langchain. Langchain pozwala tworzyć strumienie wywołań różnych narzędzi i przekazywać wyniki jednego narzędzia jako wejście do innego. W tym wypadku przekażemy zapytanie do bazy zanurzeń a następnie zapytanie wraz fragmentami z bazy przekażemy do LLMa. Taki prosty łańcuch wywołań można stworzyć korzystając z kodu poniżej."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b490dab-d206-4c2c-8259-74de8b375d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "from langchain_openai import ChatOpenAI\n",
    "import warnings \n",
    "warnings.simplefilter(\"ignore\") # API zmienia się bardzo szybko i co rusz coś staje się deprecated. Wyciszymy ostrzeżenia.\n",
    "\n",
    "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0) #Niska temperatura = mało losowości w odpowiedzi LLMa\n",
    "\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm,\n",
    "    retriever=vectordb.as_retriever()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ad01df6-b177-4ecd-9bdf-4494a8f4e820",
   "metadata": {},
   "source": [
    "Mając taki prosty łąncuch wywołań możemy zadać zapytanie do LLMa licząc, że skorzysta z wyszukanego tekstu podczas udzielania odpowiedzi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f90594ab-e46e-4354-9c0d-6a3b84089b03",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "question = \"What is an eyeball dataset?\"\n",
    "\n",
    "result = qa_chain({\"query\": question})\n",
    "print(result[\"result\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5da32e88-7dbf-4dfe-b979-49e842f6ed03",
   "metadata": {},
   "source": [
    "_Ciekawscy mogą teraz uruchomić ChatGPT w osobnym oknie i zobaczyć jak LLM odpowiedziałby na to samo pytanie bez znajomości kontekstu._\n",
    "\n",
    "Rzeczy, które mogą się wydarzyć jest znacznie więcej. Jedną z istotniejszych jest dodanie zdefiniowanego przez nas prompta. Prompt engineering jest sztuką, która potrafi znacząco wpłynąć na działanie RAG. Niekiedy prompty są baaaardzo długie, aby skutecznie nakierować LLM na to o co deweloperowi chodzi. Poniżej, poprosimy o to żeby odpowiedzi były zwięzłe i żeby zawsze kończyły się frazą \"Thanks for asking!\". Ponadto poprosimy o zwracanie dokumentów kontekstowych fraz z odpowiedzią."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82c2f665-e213-4e2c-9bfa-0294c164bc70",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "template = \"\"\"Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. Use three sentences maximum. Keep the answer as concise as possible. Always say \"thanks for asking!\" at the end of the answer. \n",
    "{context}\n",
    "Question: {question}\n",
    "Helpful Answer:\"\"\"\n",
    "QA_CHAIN_PROMPT = PromptTemplate.from_template(template)\n",
    "\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm,\n",
    "    retriever=vectordb.as_retriever(),\n",
    "    return_source_documents=True,\n",
    "    chain_type_kwargs={\"prompt\": QA_CHAIN_PROMPT}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "359e44d0-592f-4200-9172-71fa350750de",
   "metadata": {},
   "source": [
    "**Zad. 8: Ponownie zadaj to samo zapytanie. Jak zmieniła się odpowiedź? Zobacz z jakich książek i stron pochodzi kontekst.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c909170-3a9c-4edd-909b-e90f724fe1eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ecf23826-d893-4bac-8722-c0952571ec60",
   "metadata": {},
   "source": [
    "Odpowiedź w istocie jest zwięzła. Poprośmy żeby ją rozwinął."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88f25ac3-52c2-42f8-afd7-129b3f40fc3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = qa_chain({\"query\": \"Can you provide a longer response to my last question?\"})\n",
    "print(result[\"result\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fddb0d4-ed94-4414-ad72-d13e511d49d8",
   "metadata": {},
   "source": [
    "Ups. To chyba nie na temat. Sprawdźmy skąd pochodzi kontekst tej odpowiedzi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b93424fe-d03f-490e-8f8f-d85e73d1a7a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(result[\"source_documents\"][0].metadata)\n",
    "print(result[\"source_documents\"][1].metadata)\n",
    "print(result[\"source_documents\"][2].metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb86ebe7-c4f7-4ee3-b5f6-ffbd51c7535d",
   "metadata": {},
   "source": [
    "Tym razem z bazy wyciągnęliśmy fragmenty wywiadu u Lexa Friedmana. Wynika to stąd, że w obecnej formie nasz RAG nie ma pamięci. Każde zapytanie jest niezależne i nie możemy prowadzić dyskusji pogłębiającej poprzednie pytania. Zaraz to naprawimy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eca0a2e6-7741-47de-9d10-dcc4818c2908",
   "metadata": {},
   "source": [
    "## Pamięć i czat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f02544e-000e-4894-92f3-2f08309eb350",
   "metadata": {},
   "source": [
    "Jeśli zależy nam na dłuższych, wieloetpaowych rozmowach z LLMem, będziemy potrzebować pamięci. Pamięc będzie po prostu zapamiętywać zadane pytania i uzyskane odpowiedzi. Poniżej kod tworzący pamięć."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c85bf6ce-1eb2-4653-8f2a-613160e26612",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationBufferMemory\n",
    "memory = ConversationBufferMemory(\n",
    "    memory_key=\"chat_history\",\n",
    "    return_messages=True\n",
    ")\n",
    "\n",
    "memory.clear() # gdy chcemy zresetować pamięć"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18473c97-b6bd-47e5-9c76-d76f1b3f1cd7",
   "metadata": {},
   "source": [
    "Teraz zmienimy łańcuch wywołań na taki korzystający z pamięci. Uwaga, ten chain ma trochę inne nazwy pól zapytania i odpowiedzi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14391427-787c-4a69-8146-923a51e691b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import ConversationalRetrievalChain\n",
    "retriever=vectordb.as_retriever()\n",
    "chat_chain = ConversationalRetrievalChain.from_llm(\n",
    "    llm,\n",
    "    retriever=retriever,\n",
    "    memory=memory\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edfc0b7e-58a3-4926-acba-e557a5730797",
   "metadata": {},
   "source": [
    "Teraz ponówmy nasz eksperyment. Zadajmy pytanie i poprośmy o rozszerzenie poprzedniej odpowiedzi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd78038c-9278-4f77-8038-ad75b1003822",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"What is an eyeball dataset?\"\n",
    "\n",
    "result = chat_chain({\"question\": question})\n",
    "print(result[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29713dbc-7250-4b03-8023-e769b8a37177",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = chat_chain({\"question\": \"Can you provide a longer response to my last question?\"})\n",
    "print(result[\"answer\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4116bec4-00db-4d68-a6ca-913bcbaf724c",
   "metadata": {},
   "source": [
    "## Ocena systemu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aea084b5-767f-4c3c-b16a-bd7a9a949ad3",
   "metadata": {},
   "source": [
    "Jako ostatni element zapoznamy się z metodami oceny systemów typu RAG. Twórcę takiego systemu może interesować na ile trafne są odpowiedzi, na ile kontekst wspiera odpowiedź i na ile kontekst pasuje do pytania. Te trzy elementy sprawdzają miary Answer Relevance, Groundedness i Context Relevance. Poniżej kod tworzący nowy chain (taki kóry pozwoli zajrzeć do kontekstu i sklei nam wszystkie fragmentu kontekstu w jeden łańcuch znaków. Następnie definicja wspomnienych trzech miar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fe73f10-bc6c-42ca-9a78-33b6bfa1ac48",
   "metadata": {},
   "outputs": [],
   "source": [
    "from trulens_eval import TruChain, Feedback, Huggingface, Tru\n",
    "from trulens_eval.schema import FeedbackResult\n",
    "from trulens_eval.feedback.provider import OpenAI\n",
    "from trulens_eval.app import App\n",
    "from trulens_eval.feedback import Groundedness\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain.schema import StrOutputParser\n",
    "import numpy as np\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "tru = Tru()\n",
    "tru.reset_database()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffe916b1-a3d1-4dc4-bb75-689783c26dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | QA_CHAIN_PROMPT\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "openai = OpenAI()\n",
    "context = App.select_context(rag_chain)\n",
    "\n",
    "\n",
    "# Groundedness\n",
    "grounded = Groundedness(groundedness_provider=OpenAI())\n",
    "f_groundedness = (\n",
    "    Feedback(grounded.groundedness_measure_with_cot_reasons, name=\"Groundedness\")\n",
    "    .on(context.collect()) # collect context chunks into a list\n",
    "    .on_output()\n",
    "    .aggregate(grounded.grounded_statements_aggregator)\n",
    ")\n",
    "# Answer Relevance\n",
    "f_qa_relevance = (\n",
    "    Feedback(openai.relevance, name=\"Answer Relevance\").on_input_output()\n",
    ")\n",
    "# Context Relevance\n",
    "f_context_relevance = (\n",
    "    Feedback(openai.qs_relevance, name=\"Context Relevance\")\n",
    "    .on_input()\n",
    "    .on(context)\n",
    "    .aggregate(np.mean)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68442d2a-6517-4346-9326-cebc0b37b74a",
   "metadata": {},
   "source": [
    "Teraz stworzymy obiekt `tru_recorder`, który będzie monitorował wszystkie zapytania i je oceniał. Zamiast testować na jednym zapytaniu przeprowadzimy eksperyment i policzymy średnią z 7 zapytań żeby ocenić nasz system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47f4d7a0-8f97-4f24-818e-ddba25910124",
   "metadata": {},
   "outputs": [],
   "source": [
    "tru_recorder = TruChain(rag_chain, app_id='ChatApplication', feedbacks=[f_qa_relevance, f_context_relevance, f_groundedness])\n",
    "\n",
    "eval_questions = [\n",
    "    'What is an eyeball dataset?',\n",
    "    'What are the keys to building a career in AI?',\n",
    "    'What is the importance of networking in AI?',\n",
    "    'How can altruism be beneficial in building a career?',\n",
    "    'What is imposter syndrome and how does it relate to AI?',\n",
    "    'What will be the impact of AGI on the world?',\n",
    "    'What is the first step to becoming good at AI?',\n",
    "]\n",
    "\n",
    "# To może trochę potrwać...\n",
    "for question in eval_questions:\n",
    "    print(question)\n",
    "    with tru_recorder as recording:\n",
    "        rag_chain.invoke(question)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94d21186-1b34-4ad9-ba5f-1906149fca5a",
   "metadata": {},
   "source": [
    "Możemy zobaczyć jak nasz system sobie radzi uruchamiając komendę `tru.get_leaderboard()`. Jeśli będziemy testować wiele wersji aplikacji (parametr `app_id`) to możemy porównywać wersje między sobą właśnie w ramach tabeli wyników."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c02f3e8e-7a52-43ee-ad7c-3ecb53ea7f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tru.get_leaderboard(app_ids=[])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "589676af-c40d-45a0-9db1-22cb7c08cba0",
   "metadata": {},
   "source": [
    "Na deser: poniższy kod uruchamia dashboard gdzie można przeanalizować każde zapytanie i uzyskane miary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90cc352a-cdc6-437e-bed9-48bb075ee0eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "tru.run_dashboard()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a925fd1-7ca7-4fe2-bf3f-4bf2c484fb4d",
   "metadata": {},
   "source": [
    "**Zad. 9: Czy jesteś w stanie stworzyć nową wersję RAG, która uzyska lepsze metryki? Spróbuj zmienić prompt, żeby odpowiedzi od LLMa były dłuższe. Spróbuj zmienić liczbę fragmentów wydobywanych z bazy. Spróbuj zmienić długość fragmentów, na które dzielone są dokumenty.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c47519e-487a-4735-aef7-de17abe9475a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
